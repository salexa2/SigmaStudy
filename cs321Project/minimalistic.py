# -*- coding: utf-8 -*-
"""Minimalistic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zVN8--zCKgYYF7pknec4zXNGGN7JjAH0
"""

#pip install pytube
#pip install SpeechRecognition
#pip install sumy
#pip install nltk
#pip install pydub
#pip install youtube_dl
#pip install transformers
#pip install vosk
#wget https://alphacephei.com/vosk/models/vosk-model-en-us-aspire-0.2.zip
#unzip vosk-model-en-us-aspire-0.2.zip
#pip install youtube-transcript-api
#pip install nltk googlesearch-python beautifulsoup4
#pip install sentencepiece

def save_transcript_to_file(transcript, filename="transcript.txt"):
    with open(filename, "w", encoding="utf-8") as f:
        f.write(transcript)

import re
from youtube_transcript_api import YouTubeTranscriptApi

def extract_video_id(url):
    video_id_regex = r"(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:watch\?v=|embed\/)|youtu\.be\/)([^&]+)"
    match = re.search(video_id_regex, url)
    if match:
        return match.group(1)
    else:
        return None

def get_transcript(video_id):
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
        transcript_text = ' '.join([entry['text'] for entry in transcript])
        return transcript_text
    except Exception as e:
        print(f"Error: {e}")
        return None

url = input("Enter the YouTube video URL: ")
video_id = extract_video_id(url)

if video_id:
    transcript = get_transcript(video_id)
    if transcript:
        print(transcript)
        save_transcript_to_file(transcript)
        print("Transcript saved to 'transcript.txt'")
    else:
        print("Could not get the transcript")
else:
    print("Invalid YouTube URL")

import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
from collections import Counter

nltk.download('punkt')
nltk.download('stopwords')

def extract_keywords(text, n=10):
    words = word_tokenize(text)
    words = [word.lower() for word in words if word.isalnum()]
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word not in stop_words]
    word_count = Counter(filtered_words)
    keywords = word_count.most_common(n)
    return [keyword[0] for keyword in keywords]

keywords = extract_keywords(transcript)
print(keywords)

from googlesearch import search

def search_web(query, num_results=10):
    links = [j for j in search(query, num_results=num_results)]
    return links

query = ' '.join(keywords)
search_results = search_web(query)
print(search_results)

import requests
from bs4 import BeautifulSoup

def scrape_content(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        content = ' '.join([p.text for p in soup.find_all('p')])
        return content
    except Exception as e:
        print(f"Error: {e}")
        return None

scraped_contents = [scrape_content(url) for url in search_results]

from transformers import T5ForConditionalGeneration, T5TokenizerFast

model_name = "t5-base"
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = T5TokenizerFast.from_pretrained(model_name)

def generate_questions_and_answers(text, max_length=512):
    inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=max_length, truncation=True)
    outputs = model.generate(inputs, max_length=max_length, min_length = 500, length_penalty=3.0, num_return_sequences=1)
    generated_text = tokenizer.decode(outputs[0])
    return generated_text

scraped_contents_str = " ".join(scraped_contents)  # or use '\n'.join(scraped_contents) to separate by newlines

questions_and_answers = generate_questions_and_answers(scraped_contents_str)

import re

filtered_output = re.sub(r'<extra_id_\d+>', '', questions_and_answers).strip()
cleaned_output = filtered_output.replace('<pad>', '').replace('</s>', '').strip()

print(cleaned_output)

import re
from nltk.tokenize import sent_tokenize
import nltk

nltk.download('punkt')  # Download the Punkt tokenizer

def get_bulleted_list(text):
    sentences = sent_tokenize(text)
    bulleted_list = []
    for sentence in sentences:
        capitalized_sentence = sentence[0].upper() + sentence[1:]
        bulleted_list.append(f"• {capitalized_sentence}\n")
    return ''.join(bulleted_list)

print(get_bulleted_list)

def save_bulleted_list_to_file(text, filename="bulleted_summary.txt"):
    sentences = sent_tokenize(text)
    with open(filename, "w") as f:
        for sentence in sentences:
            capitalized_sentence = sentence[0].upper() + sentence[1:]
            f.write(f"• {capitalized_sentence}\n")

save_bulleted_list_to_file(cleaned_output)
